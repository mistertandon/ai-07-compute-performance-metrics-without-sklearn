{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print (pd.options.display.max_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_a.csv\n",
      "5_b.csv\n",
      "5_c.csv\n",
      "5_d.csv\n",
      "5_Performance_metrics_Instructions.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 2)\n",
      "     y     proba\n",
      "0  1.0  0.637387\n",
      "1  1.0  0.635165\n",
      "2  1.0  0.766586\n",
      "3  1.0  0.724564\n",
      "4  1.0  0.889199\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('5_a.csv')\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given DataFrame column `y' contains float value, here we convert it to int\n",
    "\"\"\"\n",
    "def convert_to_int(df, col):\n",
    "    \n",
    "    df[col] = df[col].map(lambda y: int(y))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y     proba\n",
      "0  1  0.637387\n",
      "1  1  0.635165\n",
      "2  1  0.766586\n"
     ]
    }
   ],
   "source": [
    "convert_to_int(df, 'y')\n",
    "\n",
    "df_top_k_rows = df.head(n = 3)\n",
    "\n",
    "print(df_top_k_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.1 Derive the class labels from given score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_class_label(df, ref_col_name, new_col_name, thld):\n",
    "    \n",
    "    df[new_col_name] = df[ref_col_name].map(lambda prob: 1 if prob > thld else 0)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y     proba  y_hat\n",
      "0  1  0.637387      1\n",
      "1  1  0.635165      1\n",
      "2  1  0.766586      1\n"
     ]
    }
   ],
   "source": [
    "derive_class_label(df, 'proba', 'y_hat', 0.5)\n",
    "df_top_k_rows = df.head(n = 3)\n",
    "\n",
    "print(df_top_k_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.2 Compute Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1): {'name': 'tp', 'mtx_idx': (0, 0)}, (0, 1): {'name': 'fp', 'mtx_idx': (0, 1)}, (1, 0): {'name': 'fn', 'mtx_idx': (1, 0)}, (0, 0): {'name': 'tn', 'mtx_idx': (1, 1)}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "confusion_mtx_ref: \n",
    "    key: tuple(original output, predicted output)\n",
    "    \n",
    "    value: dict({})\n",
    "        name: element notation ( 'tp', 'fp', 'fn', 'tn')\n",
    "        mtx_idx: Contains matrix indices, to get confusion matx element as per notation\n",
    "\"\"\"\n",
    "\n",
    "confusion_mtx_ref = {\n",
    "    (1, 1): {'name': 'tp', 'mtx_idx': (0, 0)},\n",
    "    (0, 1): {'name': 'fp', 'mtx_idx': (0, 1)},\n",
    "    (1, 0): {'name': 'fn', 'mtx_idx': (1, 0)},\n",
    "    (0, 0): {'name': 'tn', 'mtx_idx': (1, 1)}\n",
    "}\n",
    "\n",
    "print(confusion_mtx_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': (0, 0), 'fp': (0, 1), 'fn': (1, 0), 'tn': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "def get_confusion_mtx_element_ref(confusion_mtx_ref):\n",
    "    \n",
    "    cf_mtx_elem_ref = dict()\n",
    "    for (row, col) in confusion_mtx_ref:\n",
    "\n",
    "        temp = confusion_mtx_ref.get((row, col))\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        conf_mtx_elem_not = temp.get('name')\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        conf_mtx_elem_index = temp.get('mtx_idx')\n",
    "\n",
    "        cf_mtx_elem_ref.update({conf_mtx_elem_not: conf_mtx_elem_index})\n",
    "\n",
    "    return cf_mtx_elem_ref\n",
    "\n",
    "cf_mtx_elem_ref = get_confusion_mtx_element_ref(confusion_mtx_ref)\n",
    "print(cf_mtx_elem_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1): (0, 0), (0, 1): (0, 1), (1, 0): (1, 0), (0, 0): (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "cf_mtx_elem_to_idx_mapping = {elem_nota_numeric: confusion_mtx_ref.get(elem_nota_numeric).get('mtx_idx') for elem_nota_numeric in confusion_mtx_ref}\n",
    "print(cf_mtx_elem_to_idx_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_mtx(series_i, series_ii, cf_mtx_elem_to_idx_mapping):\n",
    "    \n",
    "    confusion_mtx = np.zeros((2, 2), dtype = int)\n",
    "    \n",
    "    for index, item_y in enumerate(series_i):\n",
    "\n",
    "        item_y_hat = series_ii.get(index)\n",
    "\n",
    "        x, y = cf_mtx_elem_to_idx_mapping.get((item_y, item_y_hat))\n",
    "\n",
    "        cf_item_count = confusion_mtx[x, y]\n",
    "\n",
    "        confusion_mtx[x, y] = cf_item_count + 1\n",
    "\n",
    "    return confusion_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10000   100]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "confusion_mtx = get_confusion_mtx(df['y'], df['y_hat'], cf_mtx_elem_to_idx_mapping)\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.3 Compute F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_mtx_elements(confusion_mtx, cf_mtx_elem):\n",
    "    \"\"\"Compute confusin matrix elements i.e \n",
    "        {'tp', 'fp', 'fn', 'tn'}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    confusion_mtx: numpy 2-D array\n",
    "    cf_mtx_elem: dict()\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    cf_mtx_elem_values: dict\n",
    "        Contains mapping of confusion matrix element notation to their value (in confusion matrix itself)\n",
    "        Structure: {\n",
    "        'tp': 4, 'fp': 8, 'fn':5, 'tn': 7\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    cf_mtx_elem_values = dict()\n",
    "    for cf_mtx_elem_notation in cf_mtx_elem:\n",
    "        \"\"\"\n",
    "        cf_mtx_elem_notation: string\n",
    "            Confusion matrix element notation \n",
    "            Exp: {'tp', 'fp', 'fn', 'tn'}\n",
    "        \"\"\"\n",
    "\n",
    "        x, y = cf_mtx_elem.get(cf_mtx_elem_notation)\n",
    "        val = confusion_mtx[x, y]\n",
    "\n",
    "        cf_mtx_elem_values.update({cf_mtx_elem_notation: val})\n",
    "\n",
    "\n",
    "    return cf_mtx_elem_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "F1 score = 2 * precision * recall / (precision + recall)\n",
    "\"\"\"\n",
    "def compute_tpr_fpr(cf_mtx_elem_values):\n",
    "    \n",
    "    tp = cf_mtx_elem_values.get('tp')\n",
    "    fp = cf_mtx_elem_values.get('fp')\n",
    "    fn = cf_mtx_elem_values.get('fn')\n",
    "    tn = cf_mtx_elem_values.get('tn')\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    \n",
    "    fpr = fp / (fp + tn)\n",
    "    \n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_mtx_elem_values = compute_confusion_mtx_elements(confusion_mtx, cf_mtx_elem_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9900990099009901\n",
      "1.0\n",
      "0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "#def compute_precision_recall_f1(cf_mtx_elem_values):\n",
    "def compute_auc_terms(cf_mtx_elem_values, auc_terms):\n",
    "    \n",
    "    auc_terms_ret = dict()\n",
    "    auc_term_dict = dict()\n",
    "    \n",
    "    tp = cf_mtx_elem_values.get('tp')\n",
    "    fp = cf_mtx_elem_values.get('fp')\n",
    "    fn = cf_mtx_elem_values.get('fn')\n",
    "    tn = cf_mtx_elem_values.get('tn')\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    \n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    \n",
    "    fpr = fp / (fp + tn)\n",
    "    \n",
    "    auc_term_dict.update({\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tn': tn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'tpr': tpr,\n",
    "        'fpr': fpr\n",
    "    })\n",
    "    \n",
    "    auc_terms_ret = {term: auc_term_dict.get(term) for term in auc_terms}\n",
    "    \n",
    "    return auc_terms_ret\n",
    "\n",
    "\n",
    "auc_term_obj = compute_auc_terms(cf_mtx_elem_values, ['precision', 'recall', 'f1_score'])\n",
    "print(auc_term_obj['precision'])\n",
    "print(auc_term_obj['recall'])\n",
    "print(auc_term_obj['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "auc_terms = compute_auc_terms(cf_mtx_elem_values)\n",
    "\n",
    "print(auc_terms)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.3 AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(df, col_name, order = 'DESC'):\n",
    "    \n",
    "    unique_proba = df[col_name].unique()\n",
    "    unique_proba.sort()\n",
    "    \n",
    "    if order == 'DESC':\n",
    "        unique_proba = unique_proba[::-1]\n",
    "        \n",
    "    return unique_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10100,)\n",
      "[0.89996535 0.89982831 0.89982485 0.89981181 0.89976788]\n"
     ]
    }
   ],
   "source": [
    "unique_proba = get_unique_values(df, 'proba')\n",
    "\n",
    "unique_proba_shape = unique_proba.shape\n",
    "\n",
    "print(type(unique_proba))\n",
    "print(unique_proba_shape)\n",
    "print(unique_proba[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', unique_proba_shape[0] + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc_prop(df, output_prob_col, unique_proba, auc_props_list):\n",
    "\n",
    "    auc_props = dict()\n",
    "    tpr, fpr = list(), list()\n",
    "    for index, thld in enumerate(unique_proba):\n",
    "\n",
    "        col_name, confusion_mtx_ii = '', ''\n",
    "        cf_mtx_elem_values_ii, auc_term_obj = dict(), dict()\n",
    "\n",
    "        col_name = \"thld_{}\".format(index)\n",
    "        df[(col_name, thld)] = df[output_prob_col].map(lambda proba_val: 1 if proba_val > thld else 0)\n",
    "\n",
    "        confusion_mtx_ii = get_confusion_mtx(df['y'], df[(col_name, thld)], cf_mtx_elem_to_idx_mapping)\n",
    "\n",
    "        cf_mtx_elem_values_ii = compute_confusion_mtx_elements(confusion_mtx_ii, cf_mtx_elem_ref)\n",
    "\n",
    "        auc_term_obj = compute_auc_terms(cf_mtx_elem_values_ii, auc_props_list)\n",
    "\n",
    "        auc_props.update({(col_name, thld): auc_term_obj})\n",
    "\n",
    "    return auc_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_curve_lists(auc_props, auc_props_list):\n",
    "    \n",
    "    prop_i_list, prop_ii_list = list(), list()\n",
    "    for col_name in auc_props:\n",
    "        \n",
    "        prop_i = auc_props.get(col_name).get(auc_props_list[0])\n",
    "        prop_ii = auc_props.get(col_name).get(auc_props_list[1])\n",
    "        \n",
    "        \n",
    "        prop_i_list.append(prop_i)\n",
    "        prop_ii_list.append(prop_ii)\n",
    "        \n",
    "    return prop_i_list, prop_ii_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc_props = compute_auc_prop(df, 'proba', unique_proba, ['tpr', 'fpr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tpr, fpr = get_auc_curve_lists(auc_props, ['tpr', 'fpr'])\n",
    "print(tpr)\n",
    "print(fpr)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "auc_score = np.trapz(tpr, fpr)\n",
    "print(auc_score)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2852, 2)\n",
      "   y      prob\n",
      "0  0  0.458521\n",
      "1  0  0.505037\n",
      "2  0  0.418652\n",
      "3  0  0.412057\n",
      "4  0  0.375579\n"
     ]
    }
   ],
   "source": [
    "# write your code\n",
    "df_iii = pd.read_csv(\"5_c.csv\")\n",
    "\n",
    "print(df_iii.shape)\n",
    "print(df_iii.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_int(df_iii, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2791,)\n",
      "[0.9577468  0.95143692 0.94863779 0.94409361 0.94111318]\n"
     ]
    }
   ],
   "source": [
    "unique_proba_iii = get_unique_values(df_iii, 'prob')\n",
    "\n",
    "print(unique_proba_iii.shape)\n",
    "print(unique_proba_iii[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', df_iii.shape[0] + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_props_iii = compute_auc_prop(df_iii, 'prob', unique_proba_iii, ['fn', 'fp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2852, 2793)\n"
     ]
    }
   ],
   "source": [
    "print(df_iii.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mtx_scores(auc_props):\n",
    "    \n",
    "    mtx_score = dict()\n",
    "    for thld_col in auc_props:\n",
    "\n",
    "        fn = auc_props.get(thld_col).get('fn')\n",
    "        fp = auc_props.get(thld_col).get('fp')\n",
    "        \n",
    "        score = 500 * fn + 100 * fp\n",
    "        \n",
    "        mtx_score.update({thld_col: score})\n",
    "    \n",
    "    return mtx_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_score = get_mtx_scores(auc_props_iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141000\n"
     ]
    }
   ],
   "source": [
    "key_min = min(mtx_score.keys(), key=(lambda k: mtx_score[k]))\n",
    "print(mtx_score[key_min])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y   pred\n",
      "0  101.0  100.0\n",
      "1  120.0  100.0\n",
      "2  131.0  113.0\n",
      "3  164.0  125.0\n",
      "4  154.0  152.0\n"
     ]
    }
   ],
   "source": [
    "df_iv = pd.read_csv(\"5_d.csv\")\n",
    "print(df_iv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4.1 Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.16569974554707\n"
     ]
    }
   ],
   "source": [
    "mse = 0\n",
    "for output, pred_output in zip(df_iv['y'], df_iv['pred']):\n",
    "\n",
    "    mse += np.power((output - pred_output), 2)\n",
    "    \n",
    "mse = np.divide(mse, len(df_iv['y']))\n",
    "print(mse)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
